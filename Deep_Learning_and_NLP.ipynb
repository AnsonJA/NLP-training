{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning and NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyN1sF/AcNQ4toBcW9PIo2nt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzhenCloudAI/NLP-training/blob/master/Deep_Learning_and_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUoD449le6l5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsomrAY5uc5F",
        "colab_type": "code",
        "outputId": "c97e45ef-dcb4-43c0-fc22-f87aa2585526",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 114
        }
      },
      "source": [
        "################\n",
        "# get the data #\n",
        "################\n",
        "# code source https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub\n",
        "pd.set_option('display.max_colwidth', 2000)\n",
        "\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),  \"aclImdb\", \"test\"))\n",
        "  return train_df, test_df\n",
        "\n",
        "train, test = download_and_load_datasets()\n",
        "\n",
        "train['dataSplit']='train'\n",
        "test['dataSplit']='test'\n",
        "mydata = pd.concat([train, test], axis=0)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 10s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yb2fC8sxP49",
        "colab_type": "code",
        "outputId": "0c3b0c0f-a1e9-448c-aebf-34c6069c141a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "######################\n",
        "# data preprocessing #\n",
        "######################\n",
        "\n",
        "# https://github.com/bryan-c-castillo/MLADS-TextEmbedding-Bert-Elmo-Tutorial\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "max_words = 128\n",
        "\n",
        "text_http_re  = re.compile(r'http\\S+')\n",
        "text_digit_re = re.compile(r'[0-9]')\n",
        "text_html_re  = re.compile(r'<[^>]{0,20}>')\n",
        "text_punc_re  = re.compile('[' + re.escape('\\'!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~') + ']')\n",
        "text_ws_re    = re.compile('\\s+')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text_http_re.sub('', text)\n",
        "    text = text_html_re.sub('', text)\n",
        "    text = text_digit_re.sub(' ', text)\n",
        "    text = text_punc_re.sub('', text)\n",
        "    text = text_ws_re.sub(' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def create_lemmatizer_spacy():\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    def lemmatize(text):\n",
        "        return ' '.join([token.lemma_ for token in nlp(text)][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "def create_lemmatizer_nltk():\n",
        "    from nltk.stem import WordNetLemmatizer \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    def lemmatize(text):\n",
        "        return ' '.join([lemmatizer.lemmatize(w) for w in text.split()][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "# Setup a lemmatize function, spacy.load may fail on windows for en.\n",
        "try:\n",
        "    lemmatize = create_lemmatizer_spacy()\n",
        "except:\n",
        "    print(\"Using nltk for lemmatization.\")\n",
        "    lemmatize = create_lemmatizer_nltk()\n",
        "            \n",
        "def process_text(text):\n",
        "    return lemmatize(clean_text(text))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Using nltk for lemmatization.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUz_EBWzwt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata['clean_review'] = mydata.sentence.apply(process_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccTnfQQQZqXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########\n",
        "#TF-IDF #\n",
        "######### "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDb7Cb2C4KS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vec(mydata):\n",
        "    tfidf = TfidfVectorizer(min_df=100, max_df=0.2, ngram_range=(1,1))\n",
        "    #min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "    #max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. \n",
        "    #ngram_range: unigram\n",
        "    tfidf.fit(mydata[\"sentence\"])\n",
        "    features = tfidf.transform(mydata[\"sentence\"])\n",
        "    return pd.DataFrame(features.todense(), columns = tfidf.get_feature_names())\n",
        "\n",
        "tfidf_feature=tfidf_vec(mydata)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWaliRHscx6j",
        "colab_type": "code",
        "outputId": "372fd2d8-d486-4ac7-a7e0-c36b273ba6e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "tfidf_feature.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>13th</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>1930</th>\n",
              "      <th>1930s</th>\n",
              "      <th>1939</th>\n",
              "      <th>1940</th>\n",
              "      <th>1940s</th>\n",
              "      <th>1945</th>\n",
              "      <th>1950</th>\n",
              "      <th>1950s</th>\n",
              "      <th>1959</th>\n",
              "      <th>1960</th>\n",
              "      <th>1960s</th>\n",
              "      <th>1968</th>\n",
              "      <th>1969</th>\n",
              "      <th>1970</th>\n",
              "      <th>1970s</th>\n",
              "      <th>1971</th>\n",
              "      <th>1972</th>\n",
              "      <th>1973</th>\n",
              "      <th>1974</th>\n",
              "      <th>1975</th>\n",
              "      <th>1976</th>\n",
              "      <th>1977</th>\n",
              "      <th>1978</th>\n",
              "      <th>1979</th>\n",
              "      <th>1980</th>\n",
              "      <th>...</th>\n",
              "      <th>wrenching</th>\n",
              "      <th>wrestling</th>\n",
              "      <th>wretched</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writers</th>\n",
              "      <th>writes</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wtf</th>\n",
              "      <th>wwii</th>\n",
              "      <th>www</th>\n",
              "      <th>ya</th>\n",
              "      <th>yard</th>\n",
              "      <th>yawn</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yelling</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yep</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngest</th>\n",
              "      <th>your</th>\n",
              "      <th>yours</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youth</th>\n",
              "      <th>youthful</th>\n",
              "      <th>youtube</th>\n",
              "      <th>zero</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.141544</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.130554</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 6244 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    00  000   10  100  101   11  ...  youthful  youtube  zero  zombie  zombies  zone\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0      0.0   0.0     0.0      0.0   0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0      0.0   0.0     0.0      0.0   0.0\n",
              "\n",
              "[2 rows x 6244 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fydKJ_7TXkAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata=mydata.reset_index()\n",
        "tfidf_feature['polarity']=mydata['polarity']\n",
        "tfidf_feature['dataSplit']=mydata['dataSplit']\n",
        "train_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='train']\n",
        "test_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='test']\n",
        "# too many features and need to do feature selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvIW85GYe7-y",
        "colab_type": "code",
        "outputId": "92a82623-fb12-4b82-e257-14a39a6787e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# this takes a long time and I will suggest you run the code after the tutorial. \n",
        "# need to package this into a function\n",
        "from xgboost import XGBClassifier\n",
        "train_tfidf2=train_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "xgb = XGBClassifier(max_depth=6)\n",
        "xgb.fit(train_tfidf2, train_tfidf.polarity)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "test_tfidf2=test_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "predictions = xgb.predict_proba(test_tfidf2)\n",
        "score = roc_auc_score(test.polarity, predictions[:,1])\n",
        "print('prediction auc of xgb is {}.'.format(score)) #auc 0.91 not bad; "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=6,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdeXMo6ixJ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################\n",
        "# word2vec - could be useful in labelling #\n",
        "###########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogMrCRc75YmV",
        "colab_type": "code",
        "outputId": "ccd661db-a80c-4ad3-c70c-b970aac56a5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
        "print(word_vectors.similarity('happy', 'glad'))\n",
        "result = word_vectors.similar_by_word(\"angry\")\n",
        "print(\"{}: {:.4f}\".format(*result[0]))\n",
        "sim = word_vectors.n_similarity(['replace', 'disk'], ['config', 'firmware'])\n",
        "print(\"{:.4f}\".format(sim))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.783336\n",
            "furious: 0.8144\n",
            "0.3053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKCFEcvlMsvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########\n",
        "# doc2vec #\n",
        "###########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oIEbNH6CY_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_sentences(df):\n",
        "    mylabel = []\n",
        "    for index, datapoint in df.iterrows():\n",
        "        token = re.findall(w,datapoint[\"sentence\"]\n",
        "        token2 = ' '.join(token).split() \n",
        "        mylabel.append(TaggedDocument(words=token2, tags=['S_%s' %index]))\n",
        "    return labeled_sentences\n",
        "\n",
        "def get_vectors(model, corpus, size):\n",
        "    # get vectors from doc2vec \n",
        "    vecs = np.zeros((len(corpus), size))\n",
        "    n = 0\n",
        "    for i in corpus.index:\n",
        "        prefix = 'SENT_' + str(i)\n",
        "        vecs[n] = model.docvecs[prefix]\n",
        "        n += 1\n",
        "    return vecs\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0xLaZtTx4b7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w = re.compile(\"\\w+\",re.I)\n",
        "sen = label_sentences(mydata) \n",
        "model = Doc2Vec(sen, dm=0, vector_size=20, window=5, min_count=3, epochs=20, negative=5, workers=4) #negative 5-10; window 5-10; negative sampling 5 is the best; \n",
        "train_vecs_dbow = get_vectors(model, mydata['sentence'], 20) \n",
        "out=pd.DataFrame(train_vecs_dbow)\n",
        "out.columns=['var'+str(i) for i in range(out.shape[1])]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3kDWQJkHtuJ",
        "colab_type": "code",
        "outputId": "0f73c592-2e35-45be-89f8-5abb3dc928f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        }
      },
      "source": [
        "out.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var0</th>\n",
              "      <th>var1</th>\n",
              "      <th>var2</th>\n",
              "      <th>var3</th>\n",
              "      <th>var4</th>\n",
              "      <th>var5</th>\n",
              "      <th>var6</th>\n",
              "      <th>var7</th>\n",
              "      <th>var8</th>\n",
              "      <th>var9</th>\n",
              "      <th>var10</th>\n",
              "      <th>var11</th>\n",
              "      <th>var12</th>\n",
              "      <th>var13</th>\n",
              "      <th>var14</th>\n",
              "      <th>var15</th>\n",
              "      <th>var16</th>\n",
              "      <th>var17</th>\n",
              "      <th>var18</th>\n",
              "      <th>var19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.620068</td>\n",
              "      <td>0.653183</td>\n",
              "      <td>-0.037910</td>\n",
              "      <td>-0.191700</td>\n",
              "      <td>0.791465</td>\n",
              "      <td>-0.28859</td>\n",
              "      <td>-0.236521</td>\n",
              "      <td>-0.124031</td>\n",
              "      <td>-0.043203</td>\n",
              "      <td>-0.564868</td>\n",
              "      <td>0.577796</td>\n",
              "      <td>-0.904286</td>\n",
              "      <td>-0.757336</td>\n",
              "      <td>0.090010</td>\n",
              "      <td>0.264907</td>\n",
              "      <td>1.209347</td>\n",
              "      <td>0.740613</td>\n",
              "      <td>-0.203969</td>\n",
              "      <td>0.334598</td>\n",
              "      <td>-0.943984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.379895</td>\n",
              "      <td>1.356325</td>\n",
              "      <td>-0.827584</td>\n",
              "      <td>0.207875</td>\n",
              "      <td>0.101644</td>\n",
              "      <td>0.23736</td>\n",
              "      <td>-0.045564</td>\n",
              "      <td>-0.335384</td>\n",
              "      <td>0.157258</td>\n",
              "      <td>-0.896710</td>\n",
              "      <td>-0.302423</td>\n",
              "      <td>-1.200692</td>\n",
              "      <td>0.172523</td>\n",
              "      <td>-0.139115</td>\n",
              "      <td>-0.020751</td>\n",
              "      <td>0.832373</td>\n",
              "      <td>-0.253596</td>\n",
              "      <td>-0.461508</td>\n",
              "      <td>0.373627</td>\n",
              "      <td>0.086877</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       var0      var1      var2  ...     var17     var18     var19\n",
              "0  0.620068  0.653183 -0.037910  ... -0.203969  0.334598 -0.943984\n",
              "1  0.379895  1.356325 -0.827584  ... -0.461508  0.373627  0.086877\n",
              "\n",
              "[2 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN-JZ92sx6hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out['polarity']=mydata['polarity']\n",
        "out['dataSplit']=mydata['dataSplit']\n",
        "train_doc2vec=out.loc[out['dataSplit']=='train']\n",
        "test_doc2vec=out.loc[out['dataSplit']=='test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z_vTrR6KyQc",
        "colab_type": "code",
        "outputId": "50e49c71-d576-4d80-8116-a6664f3bd32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "from xgboost import XGBClassifier\n",
        "train_doc2vec2=train_doc2vec.drop(['dataSplit', 'polarity'], axis=1)\n",
        "xgb = XGBClassifier(max_depth=5)\n",
        "xgb.fit(train_doc2vec2, train_doc2vec.polarity)\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "test_doc2vec2=test_doc2vec.drop(['dataSplit', 'polarity'], axis=1)\n",
        "predictions = xgb.predict_proba(test_doc2vec2)\n",
        "score = roc_auc_score(test_doc2vec.polarity, predictions[:,1])\n",
        "print('prediction auc of xgb is {}.'.format(score)) #auc 0.91 not bad; \n",
        "#0.95, pretty good!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}