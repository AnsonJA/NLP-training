{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning and NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP+U5yphjMvK5d4MuqUba5d",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzhenCloudAI/NLP-training/blob/master/Deep_Learning_and_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUoD449le6l5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "d425a1c6-e94c-44f7-b116-f6fac4ce7c1a"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsomrAY5uc5F",
        "colab_type": "code",
        "outputId": "679c280b-d93c-4045-dfa5-1c2a85b63e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "################\n",
        "# get the data #\n",
        "################\n",
        "# code source https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub\n",
        "pd.set_option('display.max_colwidth', 2000)\n",
        "\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),  \"aclImdb\", \"test\"))\n",
        "  return train_df, test_df\n",
        "\n",
        "train, test = download_and_load_datasets()\n",
        "\n",
        "train['dataSplit']='train'\n",
        "test['dataSplit']='test'\n",
        "mydata = pd.concat([train, test], axis=0)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yb2fC8sxP49",
        "colab_type": "code",
        "outputId": "f49e9fcb-ee4f-467d-a6cb-ea61dd34d566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "######################\n",
        "# data preprocessing #\n",
        "######################\n",
        "\n",
        "# https://github.com/bryan-c-castillo/MLADS-TextEmbedding-Bert-Elmo-Tutorial\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "max_words = 128\n",
        "\n",
        "text_http_re  = re.compile(r'http\\S+')\n",
        "text_digit_re = re.compile(r'[0-9]')\n",
        "text_html_re  = re.compile(r'<[^>]{0,20}>')\n",
        "text_punc_re  = re.compile('[' + re.escape('\\'!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~') + ']')\n",
        "text_ws_re    = re.compile('\\s+')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text_http_re.sub('', text)\n",
        "    text = text_html_re.sub('', text)\n",
        "    text = text_digit_re.sub(' ', text)\n",
        "    text = text_punc_re.sub('', text)\n",
        "    text = text_ws_re.sub(' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def create_lemmatizer_spacy():\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    def lemmatize(text):\n",
        "        return ' '.join([token.lemma_ for token in nlp(text)][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "def create_lemmatizer_nltk():\n",
        "    from nltk.stem import WordNetLemmatizer \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    def lemmatize(text):\n",
        "        return ' '.join([lemmatizer.lemmatize(w) for w in text.split()][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "# Setup a lemmatize function, spacy.load may fail on windows for en.\n",
        "try:\n",
        "    lemmatize = create_lemmatizer_spacy()\n",
        "except:\n",
        "    print(\"Using nltk for lemmatization.\")\n",
        "    lemmatize = create_lemmatizer_nltk()\n",
        "            \n",
        "def process_text(text):\n",
        "    return lemmatize(clean_text(text))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Using nltk for lemmatization.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUz_EBWzwt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata['clean_review'] = mydata.sentence.apply(process_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccTnfQQQZqXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########\n",
        "#TF-IDF #\n",
        "######### "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDb7Cb2C4KS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vec(mydata):\n",
        "    tfidf = TfidfVectorizer(min_df=100, max_df=0.2, ngram_range=(1,1))\n",
        "    #min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "    #max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. \n",
        "    #ngram_range: unigram\n",
        "    tfidf.fit(mydata[\"clean_review\"])\n",
        "    features = tfidf.transform(mydata[\"clean_review\"])\n",
        "    return pd.DataFrame(features.todense(), columns = tfidf.get_feature_names())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fpZ8ZA-Kwb2u",
        "colab": {}
      },
      "source": [
        "def xgboostFun(train_tfidf, test_tfidf) : \n",
        "  train_tfidf2=train_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "  xgb = XGBClassifier(max_depth=6)\n",
        "  xgb.fit(train_tfidf2, train_tfidf.polarity)\n",
        "\n",
        "  test_tfidf2=test_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "  predictions = xgb.predict_proba(test_tfidf2)\n",
        "  return roc_auc_score(test.polarity, predictions[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fxxA4LztfRp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "92442391-3303-4216-8438-cc4ae7b3415c"
      },
      "source": [
        "tfidf_feature=tfidf_vec(mydata)\n",
        "tfidf_feature.head(2)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>00</th>\n",
              "      <th>000</th>\n",
              "      <th>10</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>13th</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>1930</th>\n",
              "      <th>1930s</th>\n",
              "      <th>1939</th>\n",
              "      <th>1940</th>\n",
              "      <th>1940s</th>\n",
              "      <th>1945</th>\n",
              "      <th>1950</th>\n",
              "      <th>1950s</th>\n",
              "      <th>1959</th>\n",
              "      <th>1960</th>\n",
              "      <th>1960s</th>\n",
              "      <th>1968</th>\n",
              "      <th>1969</th>\n",
              "      <th>1970</th>\n",
              "      <th>1970s</th>\n",
              "      <th>1971</th>\n",
              "      <th>1972</th>\n",
              "      <th>1973</th>\n",
              "      <th>1974</th>\n",
              "      <th>1975</th>\n",
              "      <th>1976</th>\n",
              "      <th>1977</th>\n",
              "      <th>1978</th>\n",
              "      <th>1979</th>\n",
              "      <th>1980</th>\n",
              "      <th>...</th>\n",
              "      <th>wrenching</th>\n",
              "      <th>wrestling</th>\n",
              "      <th>wretched</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writers</th>\n",
              "      <th>writes</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wtf</th>\n",
              "      <th>wwii</th>\n",
              "      <th>www</th>\n",
              "      <th>ya</th>\n",
              "      <th>yard</th>\n",
              "      <th>yawn</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>years</th>\n",
              "      <th>yelling</th>\n",
              "      <th>yellow</th>\n",
              "      <th>yep</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>youngest</th>\n",
              "      <th>your</th>\n",
              "      <th>yours</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youth</th>\n",
              "      <th>youthful</th>\n",
              "      <th>youtube</th>\n",
              "      <th>zero</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10078</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 6244 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    00  000   10  100  101   11  ...  youthful  youtube  zero  zombie  zombies  zone\n",
              "0  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0      0.0   0.0     0.0      0.0   0.0\n",
              "1  0.0  0.0  0.0  0.0  0.0  0.0  ...       0.0      0.0   0.0     0.0      0.0   0.0\n",
              "\n",
              "[2 rows x 6244 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fydKJ_7TXkAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata=mydata.reset_index()\n",
        "tfidf_feature['polarity']=mydata['polarity']\n",
        "tfidf_feature['dataSplit']=mydata['dataSplit']\n",
        "train_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='train']\n",
        "test_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='test']\n",
        "# too many features and need to do feature selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvIW85GYe7-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this takes a long time and I will suggest you run the code after the tutorial. \n",
        "tfidfAUC=xgboostFun(train_tfidf, test_tfidf) #auc0.91"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdeXMo6ixJ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################\n",
        "# word2vec - could be useful in labelling #\n",
        "###########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tDQOXLPzDOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2vecFun(mydata, wordToCheck): \n",
        "  # this is to train word2vec on movie data to get similar word\n",
        "  # to get the word format\n",
        "  wordList=list()\n",
        "  lines=mydata.clean_review.values.tolist()\n",
        "  for line in lines: \n",
        "    token=line.lower().split()\n",
        "    wordList.append(token)\n",
        "\n",
        "  model=gensim.models.Word2Vec(sentences=wordList, size=50, window=5, min_count=10)\n",
        "  words=list(model.wv.vocab)\n",
        "  out=model.wv.most_similar(wordToCheck)\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3BqaHhKxkiC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "8f5a9ead-ce04-4645-e93d-dea7b3952daa"
      },
      "source": [
        "word2vecFun(mydata, 'outstanding')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('excellent', 0.889236330986023),\n",
              " ('exceptional', 0.8805518746376038),\n",
              " ('superb', 0.8580080270767212),\n",
              " ('stellar', 0.8410035371780396),\n",
              " ('masterful', 0.829212486743927),\n",
              " ('phenomenal', 0.8242124319076538),\n",
              " ('excellent,', 0.8192064166069031),\n",
              " ('magnificent', 0.8180720806121826),\n",
              " ('marvelous', 0.8142855167388916),\n",
              " ('terrific', 0.8116052150726318)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3uuzVzNz7vL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "90f3ee3f-4bb6-4081-a115-6d3c84242209"
      },
      "source": [
        "# # load pre-trained word-vectors from gensim-data\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")  \n",
        "print(word_vectors.similarity('happy', 'glad'))\n",
        "result = word_vectors.similar_by_word(\"angry\")\n",
        "print(result)\n",
        "sim = word_vectors.n_similarity(['replace', 'disk'], ['config', 'firmware'])\n",
        "print(sim)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.783336\n",
            "[('furious', 0.8143535256385803), ('outraged', 0.7746474146842957), ('enraged', 0.7717769742012024), ('irate', 0.7348275184631348), ('frustrated', 0.7237215042114258), ('angered', 0.7120067477226257), ('frightened', 0.7118747234344482), ('shocked', 0.69240403175354), ('fearful', 0.6863806843757629), ('crowd', 0.6827539205551147)]\n",
            "0.30528376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKCFEcvlMsvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########\n",
        "# doc2vec #\n",
        "###########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oIEbNH6CY_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_sentences(df):\n",
        "   docs=df.clean_review\n",
        "   tagged_data = [TaggedDocument(words=d.lower().split(), tags=[str(i)]) for i, d in enumerate(docs)]\n",
        "   return tagged_data\n",
        "\n",
        "def get_vectors(model, corpus, size):\n",
        "    # get vectors from doc2vec \n",
        "    vecs = np.zeros((len(corpus), size))\n",
        "    n = 0\n",
        "    for i in corpus.index:\n",
        "        prefix = 'SENT_' + str(i)\n",
        "        vecs[n] = model.docvecs[prefix]\n",
        "        n += 1\n",
        "    return vecs\n",
        "\n",
        "def doc2vecFun(mydata): \n",
        "  sen = label_sentences(mydata) \n",
        "  model = Doc2Vec(sen, dm=0, vector_size=20, window=5, min_count=3)  \n",
        "  train_vecs_dbow = get_vectors(model, mydata['clean_review'], 20) \n",
        "  out=pd.DataFrame(train_vecs_dbow)\n",
        "  out.columns=['var'+str(i) for i in range(out.shape[1])]\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN-JZ92sx6hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out=doc2vecFun(mydata)\n",
        "out['polarity']=mydata['polarity']\n",
        "out['dataSplit']=mydata['dataSplit']\n",
        "train_doc2vec=out.loc[out['dataSplit']=='train']\n",
        "test_doc2vec=out.loc[out['dataSplit']=='test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z_vTrR6KyQc",
        "colab_type": "code",
        "outputId": "50e49c71-d576-4d80-8116-a6664f3bd32c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "doc2vecAUC=xgboostFun(train_doc2vec, test_doc2vec)\n",
        "#auc 0.95, pretty good!"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "              colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
              "              learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
              "              min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
              "              nthread=None, objective='binary:logistic', random_state=0,\n",
              "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "              silent=None, subsample=1, verbosity=1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    }
  ]
}