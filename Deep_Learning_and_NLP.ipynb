{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning and NLP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMzuXq10OGxVE/QN82fXTES",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lzhenCloudAI/NLP-training/blob/master/Deep_Learning_and_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUoD449le6l5",
        "colab_type": "code",
        "outputId": "1cd6b657-a560-42a3-a239-aa67047cc36e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsomrAY5uc5F",
        "colab_type": "code",
        "outputId": "9125ca70-ec31-49ce-a22f-87e1059887c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "################\n",
        "# get the data #\n",
        "################\n",
        "# code source https://www.tensorflow.org/hub/tutorials/text_classification_with_tf_hub\n",
        "pd.set_option('display.max_colwidth', 2000)\n",
        "\n",
        "def load_directory_data(directory):\n",
        "  data = {}\n",
        "  data[\"sentence\"] = []\n",
        "  data[\"sentiment\"] = []\n",
        "  for file_path in os.listdir(directory):\n",
        "    with tf.io.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
        "      data[\"sentence\"].append(f.read())\n",
        "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
        "  return pd.DataFrame.from_dict(data)\n",
        "\n",
        "# Merge positive and negative examples, add a polarity column and shuffle.\n",
        "def load_dataset(directory):\n",
        "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
        "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
        "  pos_df[\"polarity\"] = 1\n",
        "  neg_df[\"polarity\"] = 0\n",
        "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Download and process the dataset files.\n",
        "def download_and_load_datasets(force_download=False):\n",
        "  dataset = tf.keras.utils.get_file(\n",
        "      fname=\"aclImdb.tar.gz\", \n",
        "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
        "      extract=True)\n",
        "  \n",
        "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \"aclImdb\", \"train\"))\n",
        "  test_df = load_dataset(os.path.join(os.path.dirname(dataset),  \"aclImdb\", \"test\"))\n",
        "  return train_df, test_df\n",
        "\n",
        "train, test = download_and_load_datasets()\n",
        "\n",
        "train['dataSplit']='train'\n",
        "test['dataSplit']='test'\n",
        "mydata = pd.concat([train, test], axis=0)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "84131840/84125825 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yb2fC8sxP49",
        "colab_type": "code",
        "outputId": "ebce54cd-8bad-4046-f5b5-636d8ba94236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "######################\n",
        "# data preprocessing #\n",
        "######################\n",
        "\n",
        "# https://github.com/bryan-c-castillo/MLADS-TextEmbedding-Bert-Elmo-Tutorial\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "max_words = 128\n",
        "\n",
        "text_http_re  = re.compile(r'http\\S+') #remove url\n",
        "text_digit_re = re.compile(r'[0-9]')\n",
        "text_html_re  = re.compile(r'<[^>]{0,20}>') \n",
        "text_punc_re  = re.compile('[' + re.escape('\\'!\"#$%&()*+-/:;<=>?@[\\\\]^_`{|}~') + ']')\n",
        "text_ws_re    = re.compile('\\s+') #blank\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = text_http_re.sub('', text)\n",
        "    text = text_html_re.sub('', text)\n",
        "    text = text_digit_re.sub(' ', text)\n",
        "    text = text_punc_re.sub('', text)\n",
        "    text = text_ws_re.sub(' ', text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "def create_lemmatizer_spacy():\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    def lemmatize(text):\n",
        "        return ' '.join([token.lemma_ for token in nlp(text)][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "def create_lemmatizer_nltk():\n",
        "    from nltk.stem import WordNetLemmatizer \n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    def lemmatize(text):\n",
        "        return ' '.join([lemmatizer.lemmatize(w) for w in text.split()][0:max_words])\n",
        "    \n",
        "    return lemmatize\n",
        "\n",
        "# Setup a lemmatize function, spacy.load may fail on windows for en.\n",
        "try:\n",
        "    lemmatize = create_lemmatizer_spacy()\n",
        "except:\n",
        "    print(\"Using nltk for lemmatization.\")\n",
        "    lemmatize = create_lemmatizer_nltk()\n",
        "            \n",
        "def process_text(text):\n",
        "    return lemmatize(clean_text(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Using nltk for lemmatization.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUz_EBWzwt3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata['clean_review'] = mydata.sentence.apply(process_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccTnfQQQZqXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#########\n",
        "#TF-IDF #\n",
        "######### "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDb7Cb2C4KS4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_vec(mydata):\n",
        "    tfidf = TfidfVectorizer(min_df=100, max_df=0.2, ngram_range=(1,1))\n",
        "    #min_df: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold.\n",
        "    #max_df: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold. \n",
        "    #ngram_range: unigram\n",
        "    tfidf.fit(mydata[\"clean_review\"])\n",
        "    features = tfidf.transform(mydata[\"clean_review\"])\n",
        "    return pd.DataFrame(features.todense(), columns = tfidf.get_feature_names())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fpZ8ZA-Kwb2u",
        "colab": {}
      },
      "source": [
        "def xgboostFun(train_tfidf, test_tfidf) : \n",
        "  train_tfidf2=train_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "  xgb = XGBClassifier(max_depth=6)\n",
        "  xgb.fit(train_tfidf2, train_tfidf.polarity)\n",
        "\n",
        "  test_tfidf2=test_tfidf.drop(['dataSplit', 'polarity'], axis=1)\n",
        "  predictions = xgb.predict_proba(test_tfidf2)\n",
        "  return roc_auc_score(test.polarity, predictions[:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fxxA4LztfRp",
        "colab_type": "code",
        "outputId": "0f5b5514-6964-4676-b6e2-c61b876d8814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "source": [
        "tfidf_feature=tfidf_vec(mydata)\n",
        "tfidf_feature.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>abandoned</th>\n",
              "      <th>abc</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>above</th>\n",
              "      <th>absolute</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absurd</th>\n",
              "      <th>abuse</th>\n",
              "      <th>abysmal</th>\n",
              "      <th>academy</th>\n",
              "      <th>accent</th>\n",
              "      <th>accept</th>\n",
              "      <th>acceptable</th>\n",
              "      <th>accepted</th>\n",
              "      <th>accident</th>\n",
              "      <th>accidentally</th>\n",
              "      <th>accomplished</th>\n",
              "      <th>according</th>\n",
              "      <th>account</th>\n",
              "      <th>accurate</th>\n",
              "      <th>accused</th>\n",
              "      <th>achieve</th>\n",
              "      <th>achieved</th>\n",
              "      <th>achievement</th>\n",
              "      <th>across</th>\n",
              "      <th>act</th>\n",
              "      <th>acted</th>\n",
              "      <th>acting</th>\n",
              "      <th>action</th>\n",
              "      <th>activity</th>\n",
              "      <th>actor</th>\n",
              "      <th>actors</th>\n",
              "      <th>actress</th>\n",
              "      <th>actual</th>\n",
              "      <th>actually</th>\n",
              "      <th>ad</th>\n",
              "      <th>adam</th>\n",
              "      <th>adaptation</th>\n",
              "      <th>adapted</th>\n",
              "      <th>...</th>\n",
              "      <th>worst</th>\n",
              "      <th>worth</th>\n",
              "      <th>worthless</th>\n",
              "      <th>worthwhile</th>\n",
              "      <th>worthy</th>\n",
              "      <th>would</th>\n",
              "      <th>wouldnt</th>\n",
              "      <th>wouldve</th>\n",
              "      <th>wow</th>\n",
              "      <th>wreck</th>\n",
              "      <th>write</th>\n",
              "      <th>writer</th>\n",
              "      <th>writerdirector</th>\n",
              "      <th>writers</th>\n",
              "      <th>writing</th>\n",
              "      <th>written</th>\n",
              "      <th>wrong</th>\n",
              "      <th>wrote</th>\n",
              "      <th>wwii</th>\n",
              "      <th>yeah</th>\n",
              "      <th>year</th>\n",
              "      <th>yearold</th>\n",
              "      <th>years</th>\n",
              "      <th>yes</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yet</th>\n",
              "      <th>york</th>\n",
              "      <th>youd</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>younger</th>\n",
              "      <th>your</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youth</th>\n",
              "      <th>youve</th>\n",
              "      <th>zero</th>\n",
              "      <th>zombie</th>\n",
              "      <th>zombies</th>\n",
              "      <th>zone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.096579</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.135783</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.093105</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147884</td>\n",
              "      <td>0.098214</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.220289</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 3663 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   abandoned  abc  ability  able  above  ...  youve  zero  zombie  zombies  zone\n",
              "0        0.0  0.0      0.0   0.0    0.0  ...    0.0   0.0     0.0      0.0   0.0\n",
              "1        0.0  0.0      0.0   0.0    0.0  ...    0.0   0.0     0.0      0.0   0.0\n",
              "\n",
              "[2 rows x 3663 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fydKJ_7TXkAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mydata=mydata.reset_index()\n",
        "tfidf_feature['polarity']=mydata['polarity']\n",
        "tfidf_feature['dataSplit']=mydata['dataSplit']\n",
        "train_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='train']\n",
        "test_tfidf=tfidf_feature.loc[tfidf_feature['dataSplit']=='test']\n",
        "# too many features and need to do feature selection"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvIW85GYe7-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this takes a long time and I will suggest you run the code after the tutorial. \n",
        "tfidfAUC=xgboostFun(train_tfidf, test_tfidf) #auc0.91"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdeXMo6ixJ0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########################################\n",
        "# word2vec - could be useful in labelling #\n",
        "###########################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tDQOXLPzDOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word2vecFun(mydata, wordToCheck): \n",
        "  # this is to train word2vec on movie data to get similar word\n",
        "  # to get the word format\n",
        "  wordList=list()\n",
        "  lines=mydata.clean_review.values.tolist()\n",
        "  for line in lines: \n",
        "    token=line.lower().split()\n",
        "    wordList.append(token)\n",
        "\n",
        "  model=gensim.models.Word2Vec(sentences=wordList, size=50, window=5, min_count=10)\n",
        "  #ords below the min_count frequency are dropped before training occurs\n",
        "  words=list(model.wv.vocab)\n",
        "  out=model.wv.most_similar(wordToCheck)\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3BqaHhKxkiC",
        "colab_type": "code",
        "outputId": "8413ab02-e2c7-4654-eea0-f4f507c05144",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "word2vecFun(mydata, 'outstanding')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('excellent', 0.8888216018676758),\n",
              " ('exceptional', 0.8517456650733948),\n",
              " ('stellar', 0.8512678742408752),\n",
              " ('superb', 0.8459712862968445),\n",
              " ('magnificent', 0.8411658406257629),\n",
              " ('terrific', 0.8349108695983887),\n",
              " ('phenomenal', 0.8299832940101624),\n",
              " ('fabulous', 0.8206316828727722),\n",
              " ('marvelous', 0.8083741068840027),\n",
              " ('incredible', 0.8037787079811096)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3uuzVzNz7vL",
        "colab_type": "code",
        "outputId": "90f3ee3f-4bb6-4081-a115-6d3c84242209",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# # load pre-trained word-vectors from gensim-data\n",
        "word_vectors = api.load(\"glove-wiki-gigaword-100\")  \n",
        "print(word_vectors.similarity('happy', 'glad'))\n",
        "result = word_vectors.similar_by_word(\"angry\")\n",
        "print(result)\n",
        "sim = word_vectors.n_similarity(['replace', 'disk'], ['config', 'firmware'])\n",
        "print(sim)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n",
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.783336\n",
            "[('furious', 0.8143535256385803), ('outraged', 0.7746474146842957), ('enraged', 0.7717769742012024), ('irate', 0.7348275184631348), ('frustrated', 0.7237215042114258), ('angered', 0.7120067477226257), ('frightened', 0.7118747234344482), ('shocked', 0.69240403175354), ('fearful', 0.6863806843757629), ('crowd', 0.6827539205551147)]\n",
            "0.30528376\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKCFEcvlMsvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###########\n",
        "# doc2vec #\n",
        "###########"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oIEbNH6CY_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def label_sentences(df):\n",
        "   docs=df.clean_review\n",
        "   tagged_data = [TaggedDocument(words=d.split(), tags=['SENT_'+str(i)]) for i, d in enumerate(docs)]\n",
        "   return tagged_data\n",
        "\n",
        "def get_vectors(model, corpus, size):\n",
        "    # get vectors from doc2vec \n",
        "    vecs = np.zeros((len(corpus), size))\n",
        "    n = 0\n",
        "    for i in corpus.index:\n",
        "        prefix = 'SENT_' + str(i)\n",
        "        vecs[n] = model.docvecs[prefix]\n",
        "        n += 1\n",
        "    return vecs\n",
        "\n",
        "def doc2vecFun(mydata, n): \n",
        "  sen = label_sentences(mydata) \n",
        "  model = Doc2Vec(sen, dm=0, vector_size=n, window=5, min_count=3, negative=5, epochs=20, worker=4)  \n",
        "  #gnores all words with total frequency lower than this.\n",
        "  #negative specifies how many “noise words” should be drawn\n",
        "  train_vecs_dbow = get_vectors(model, mydata['clean_review'], n) \n",
        "  out=pd.DataFrame(train_vecs_dbow)\n",
        "  out.columns=['var'+str(i) for i in range(out.shape[1])]\n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MN-JZ92sx6hE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out=doc2vecFun(mydata, 30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DeMMug8DJiKy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out['polarity']=mydata['polarity']\n",
        "out['dataSplit']=mydata['dataSplit']\n",
        "train_doc2vec=out.loc[out['dataSplit']=='train']\n",
        "test_doc2vec=out.loc[out['dataSplit']=='test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Z_vTrR6KyQc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "doc2vecAUC=xgboostFun(train_doc2vec, test_doc2vec)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJzvlavNOQYS",
        "colab_type": "code",
        "outputId": "ce024d53-d763-4eb4-e95a-38423a94f0bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "doc2vecAUC "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8993558016000001"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    }
  ]
}